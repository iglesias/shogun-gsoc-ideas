<p>
<meta http-equiv="Content-type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="en-us" />
<title>shogun | A Large Scale Machine Learning Toolbox</title>
<meta name="ROBOTS" content="ALL" />
<meta http-equiv="imagetoolbar" content="no" />
<meta name="MSSmartTagsPreventParsing" content="true" />
<meta name="Copyright" content="This site's design and contents Copyright (c) 2007-2012  Soeren Sonnenburg" />
<meta name="keywords" content="large scale, machine learning, open source" />
<meta name="description" content="SHOGUN Large Scale Machine Learning Toolbox" />
<link href="media/css/base.css" rel="stylesheet" type="text/css" media="screen" />
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</p>
<table class="ideas" style="width: 99%;" align="center">
<tbody>
<tr>
<td>
<table>
<tbody>
<tr>
<td><img src="../../../../static/figures/shogun_logo.png" border="0" alt="SHOGUN Logo" width="50" /></td>
<td>
<h1>Ideas for Google Summer of Code 2014 Projects</h1>

Shogun is a software toolbox for machine learning algorithms, with a special focus on large-scale applications. During Google Summer of Code 2014 we are looking forward to extending our library with both state-of-the-art and fundamental machine learning algorithms. <br /><br />

Below is listed a set of suggestions for projects. If you have your own concrete idea, or want to solve a task proposed during previous years that has not been tackled yet - talk to us! We might be interested.<br /><br />

<i><a href="http://www.shogun-toolbox.org/page/Events/gsoc2011_ideas.html">GSoC 2011 ideas</a></i><br />
<i><a href="http://www.shogun-toolbox.org/page/Events/gsoc2012_ideas.html">GSoC 2012 ideas</a></i><br />
<i><a href="http://www.shogun-toolbox.org/page/Events/gsoc2013_ideas.html">GSoC 2013 ideas</a></i><br /><br />

<i><a href="http://www.shogun-toolbox.org">back to shogun toolbox homepage</a></i></td>
</tr>
</tbody>
</table>
</td>
</tr>


<tr>
<td>
<h2>Table of Contents</h2>
</td>
</tr>
<tr>
<td class="ideas1" width="100%">
<h4>Machine learning tasks</h4>
<ul>
<li><a href="#deep_learning">Essential Deep Learning Modules</a> </li>
<li><a href="#variational_learning">Variational Learning for Recommendations with Big Data</a> </li>
<li><a href="#fundamental">Fundamental Machine Learning algorithms</a> </li>
<li><a href="#one_class">Implementation of Recent One-class SVM Extensions</a> </li>
<li><a href="#fast_food">Large Scale Learning: approximative kernel expansion in loglinear time (aka Fast Food)</a> </li>
<li><a href="#mcmc">Generic Framework for Markov Chain Monte Carlo Algorithms and Stan Interface</a> </li>
<li><a href="#okl">Output Kernel Learning</a> </li>
<li><a href="#variable_interactions">Testing and Measuring Variable Interactions with Kernels</a> </li>
</ul>
</td>
</tr>

<tr>
<td>
<h2>List of Ideas</h2>
</td>
</tr>
<tr>
<td class="ideas1">
<h2>Machine learning tasks</h2>
<ul>

<!-- DEEP LEARNING PROJECT IDEA -->
<li>
<h4 id="deep_learning">Essential Deep Learning Modules</h4>
<!-- Project generics -->
<i>Mentor: Theofanis Karaletsos</i><br />
<i>Shogun co-mentor: Sergey Lisitsyn</i><br />
<i>Difficulty: <span class="medium">Medium</span></i><br />
<i>Requirements: C++, Python, machine learning</i><br />
<!-- Project description -->
<p align="justify">
Deep Learning has recently attracted a lot of attention in the Machine Learning community for its ability to learn features that allow high performance in a variety of tasks ranging from Computer Vision to Speech Recognition and Bioinformatics. The main goal of this task is to integrate essential building blocks of deep learning algorithms into Shogun. This includes Restricted Boltzmann Machines (RBM) and training algorithms, Deep Belief Networks (DBN), feed-forward networks and convolutional networks. The architecture and software design for flexible usage and adaptation of models should be created to build a foundation for integrating many more models, evaluation methods and training algorithms in the future. Speaking of details, this idea considers implementation of some software foundation for deep learning and the first few algorithms (RBMs and training, stacking of RBM’s, wake-sleep for DBN’s, discriminative fine-tuning with backprop, FFN).<br /><br />

This idea is a great chance to learn the deep learning approach and essential principles of implementing deep learning algorithms. With such a serious attention being drawn to deep learning this is an important skill for any researcher or engineer working with data. <br /><br />

References:<br />
[1] Comprehensive Python toolboxes and tutorials with code examples are available in <a href="http://deeplearning.net/software/theano/">Theano</a> and in codeboxes of Alex Krizhevsky and Nitish Srivastava.<br />
[2] <a href="http://deeplearning.net/reading-list/">Deep learning reading list</a>.
</p>
</li>

<!-- VARIATIONAL LEARNING PROJECT IDEA -->
<li>
<h4 id="variational_learning">Variational Learning for Recommendations with Big Data</h4>
<!-- Project generics -->
<i>Mentor: Mohammad Emtiyaz Khan</i><br />
<i>Shogun co-mentor: Heiko Strathmann</i><br />
<i>Difficulty: <span class="medium">Medium</span> to <span class="difficult">Difficult</span></i><br />
<i>Requirements: C++, familiarity with optimization methods, prefered (but not required) that you have basic understanding of Bayesian models (or only GPs) and variational approximations</i><br />
<i>Useful: Matlab (for reference code), Eigen3, Shogun’s GP framework</i><br />
<!-- Project description -->
<p align="justify">
The era of big data brings calls for methods that scale well. For example, websites such as Netfix and Amazon collect huge amount of data about users’ preferences. Efficient use of such data can help improve recommendations and user experience through personalization. In this project, our goal will be to develop scalable code for learning from such data.<br /><br />

We will focus on models that use matrix factorization and Gaussian processes. Matrix factorization is a very popular model for recommendation systems - in fact, in the Netflix challenge, this model had the best performance obtained by any single model! We will use variational approximations since they have potential to scale well to big data. The objective function in Bayesian models are usually intractable and variational methods instead optimize a tractable lower bound to learn the model.<br /><br />

Our main tool in this project will be the implementation of many convex optimization methods for fast variational inference. Previously, in other GSoCs, Jacob Walker and Roman Votjakov implemented a flexible framework for Gaussian process regression and classification models. We will use their initial work and extend the existing infrastructure to allow variational inference for GPs.<br />
</p>

The project time line along with estimated time and difficulty level. (Note that this is a preliminary list, which might prove to be too much.)
<ul>
<li>(2 week, difficult) Implement KL-method of Nickisch and Rasmussen 2008 for GPs.</li>
<li>(1 week, easy) Implement KL-method of Challis and Barber 2011 for GPs.</li>
<li>(1 week, easy) Implement the dual variational inference, Khan et. al. 2013 for GPs.</li>
<li>(2 weeks, difficult) Implement Stochastic method for computing inverses approximately (from Hannes’ toolbox) for GPs.</li>
<li>(1 weeks, easy) Generalize these methods to a general latent Gaussian model.</li>
<li>(1 week, easy) Implement probabilistic PCA of Tipping and Bishop 1999.</li>
<li>(2 week, difficult) Incorporate KL-method into PCA to learn from non-Gaussian data simliar to Matchbox 2009.</li>
</ul>

<p align="justify">
You will gain experience working with optimization algorithms, and get a little peek into convex optimization theory as well. You will learn about Bayesian models such as matrix factorization and Gaussian process, and variational inference. Most fun part is that you will get to play with real-world data such as for recommendation systems. You will be able to understand why these datasets are difficult, and what kind of models are useful for them.  The project is quite flexible in terms of how much can be done and offers a lot of room for sophisticated extensions while the basic tasks are not too hard. You can play a lot with these things (and we expect you to). Should be quite fun!
</p>
</li>

<!-- FUNDAMENTAL ALGORITHMS PROJECT IDEA -->
<li>
<h4 id="fundamental">Fundamental Machine Learning algorithms</h4>
<!-- Project generics -->
<i>Mentor: Fernando Iglesias</i><br />
<i>Difficulty: <span class="easy">Easy</span> to <span class="medium">Medium</span></i><br />
<i>Requirements: experience with C++, Python, Octave/Matlab, understanding of basic machine learning algorithms</i><br />
<i>Useful: experience with Eigen3</i><br />
<!-- Project description -->
<p align="justify">
The machine learning research community is fast-paced, and international conferences produce new methods every year. Nevertheless, well-established, fundamental algorithms are an indispensable tool for most practitioners as they are well understood and their performance has been proven in a widen range of applications. The goal of this project is to implement fundamental ML algorithms that Shogun currently lacks, polishing and improving the existing ones, and illustrating their use through (multi-language) examples and IPython notebooks on real-life datasets.<br /><br />

A first part of the project involves the implementation of decision trees in Shogun. The C4.5 algorithm [1] is a starting point -- we are also open to suggestions. Once a method to generate a decision tree is ready, use it to provide a random forests implementation [2]. <br /><br />

A second part is k-nearest neighbours (kNN). For fast kNN search, there is a very efficient cover tree implementation in Shogun, which is however not easy to maintain and has problems in certain contexts. An additional task here is to implement another tree data structure for fast kNN, such as kd-trees [3], whose performance is somewhat comparable to the current cover tree implementation.<br /><br />

Yet another new fundamental method we want to include in Shogun is kernel density estimation [4] (also known as Parzen windows in some domains). Such an implementation must be able to leverage the many kernels available in Shogun.<br /><br />

We would also like to have a more flexible Principal Component Analysis (PCA) and Kernel PCA (KPCA) interface. For instance, we would like to give the user the possibility to choose between using Singular Value Decomposition (SVD) or eigendecomposition depending on whether the number of feature vectors is larger than the number of dimensions, or vice versa. The current implementations could be polished as well, introducing the use of Eigen3.<br /><br />

Least-Angle Regression (LARS) and Lasso are two fundamental statistical tools for regression already implemented in Shogun. We would like to have a slightly large example in the form of a notebook that compares these regression techniques. This comparison would become broader including other methods implemented in Shogun such as Support Vector Regression and Gaussian Processes [5].<br /><br />

A very important part of this project is to illustrate how these methods should be used using IPython notebooks and examples (at least covering C++, Python, and Octave/Matlab). The student should be able to come up with (or find in textbooks, articles, etc) good-looking -- i.e. with nice figures -- examples that illustrate the concepts underlying the implemented algorithms. A large-scale classification application with kNN is another idea.<br /><br />

Suggested entrance tasks:
<ul>
<li><a href="https://github.com/shogun-toolbox/shogun/issues/1158">Investigate errors with LARS</a>.</li>
<li>Get <a href="https://github.com/shogun-toolbox/shogun/pull/1110">this ID3 implementation</a> ready to be merged.</li>
<li>Write unit tests for LARS and Lasso.</li>
<li>Write an IPython notebook illustrating and comparing LARS and Lasso. High-dimensional data could be used to show the strength of LARS.</li>
</ul><br />

References:<br />
[1] <a href="http://en.wikipedia.org/wiki/C4.5_algorithm">C4.5.</a><br />
[2] <a href="http://en.wikipedia.org/wiki/Random_forests">Random forests.</a><br />
[3] <a href="http://en.wikipedia.org/wiki/Kd_tree">KD-trees.</a><br />
[4] <a href="http://en.wikipedia.org/wiki/Kernel_density_estimation">Kernel Density Estimation</a>.<br />
[5] <a href="http://shogun-toolbox.org/static/notebook/current/gaussian_processes.html">GPs notebook</a>.
</p>
</li>

<!-- ONE-CLASS SVM PROJECT IDEA -->
<li>
<h4 id="one_class">Implementation of Recent One-class SVM Extensions</h4>
<!-- Project generics -->
<i>Mentor: Nico Goernitz</i><br />
<i>Difficulty: <span class="medium">Medium</span></i><br />
<i>Requirements: C++, Python, basic machine learning and optimization skills</i><br />
<!-- Project description -->
<p align="justify">
One-class learning, aka. density level-set estimation, anomaly detection, fraud detection, concept learning, etc., has been around for a while. Most famous algorithm are the one-class support vector machine (Schoelkopf et al, 1999) and the support vector data description (Tax and Duin, 1999). Both share common ideas and, in fact, under fairly general circumstances, are interchangeable. Due to their simplicity, they have been applied in many different areas and various, even surprising, settings (e.g. multi-class classification, just to name one that contradicts its name).<br /><br />

Semi-supervised anomaly detection (2013), latent variable support vector data description (2014) and multitask one-class support vector machines (2010) are some quite recent extensions based on one-class SVM and SVDD. Some of the above methods have multiple formulations (convex and convex approximations), while others are intrinisc non-convex.<br /><br />

Basic optimization knowledge is neccessary as well as understanding of the underlying machine learning techniques (SVMs, kernels, regularization, etc.). Implementation languages will be C++ and Python. Standalone Python implementations of the above mentioned methods will be provided.<br /><br />

References:<br />
N. Görnitz, M. Kloft, K. Rieck, U. Brefeld "Toward Supervised Anomaly Detection" Journal of Artificial Intelligence Research (JAIR), 2013.<br />
Haiqin Yang, Irwin King, Michael R. Lyu, "Multi-task Learning for One-class Classification" The 2010 International Joint Conference on Neural Networks (IJCNN)<br />
N. Görnitz, A. Porbadnigk, A. Binder, C. Sannelli, M. Braun, K.-R. Mueller, M. Kloft "Learning and Evaluation in Presence of Non-i.i.d. Label Noise" (accepted at) AISTATS, 2014
</p>
</li>

<!-- FAST FOOD PROJECT IDEA -->
<li>
<h4 id="fast_food">Large Scale Learning: approximative kernel expansion in loglinear time (aka Fast Food)</h4>
<!-- Project generics -->
<i>Mentor:  Andreas Ziehe</i><br />
<i>Shogun co-mentor: Soeren Sonnenburg</i><br />
<i>Difficulty: <span class="medium">Medium</span></i><br /> to <span class="difficult">Difficult</span></i><br />
<i>Requirements: C++, machine learning, linear algebra</i><br />
<!-- Project description -->
<p align="justify">
Large scale learning with kernels is hindered by the fact that kernel classifiers require computing kernel expansions of the form \( f(x)=\sum_{i=1}^N \alpha_i K(x,x_i) \). Obviously, the more non-zero coefficients \( \alpha_i \) there are the slower the kernel machine. Recently, progress has been made in drastically speeding up kernel machines by approximating the kernel feature space [1,2,3]. The "Fast Food" approach [3] which can be seen as a modification of the "random kitchen sinks" method offers O(n log d) computation and O(n) storage for n basis functions in d dimensions. The key to achieve these speedups is based on using the Fast Hadamard transform for fast matrix-vector multiplication [6].<br /><br />

Suggested road map:
<ul>
<li>Familiarize yourself with [1,2,3] and shogun (in particular CKernelMachine and CDotFleatures).</li>
<li>Familiarize yourself with random fourier features [2] via RandomFourierDotFeatures in shogun/features/ and RandomFourierGaussPreproc.cpp in shogun/preprocessor.</li>
<li>Familiarize yourself with the Fast Hadamard transform and its implementation [6].</li>
<li>Familiarize with [3,4,5] and implement the FastFood method. Implement a speed comparison of [1,2,3].</li>
</ul>
<br />

References:<br />
[1] Efficient Additive Kernels via Explicit Feature Maps (Vedaldi, 2011)<br />
[2] <a href="https://research.microsoft.com/apps/video/dl.aspx?id=103390&l=i">Random kitchen sinks.</a><br />
[3] <a href="http://jmlr.org/proceedings/papers/v28/le13.html">Fastfood - Computing Hilbert Space Expansions in loglinear time, Quoc Le, Tamas Sarlos, Alexander Smola; JMLR W&CP 28 (3): 244–252, 2013</a><br />
[4] <a href="http://videolectures.net/nipsworkshops2012_smola_kernel">Fast Food video lecture.</a><br />
[5] <a href="http://techtalks.tv/talks/fastfood-computing-hilbert-space-expansions-in-loglinear-time/58385/">Fast Food techtalk</a><br />
[6] <a href"http://spiral.net/software/wht.html">SPIRAL WHT Package.</a>
</p>
</li>